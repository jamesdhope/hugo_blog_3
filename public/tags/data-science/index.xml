<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data Science | James Hope</title><link>https://jamesdhope.com/tags/data-science/</link><atom:link href="https://jamesdhope.com/tags/data-science/index.xml" rel="self" type="application/rss+xml"/><description>Data Science</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Wed, 04 Dec 2024 00:00:00 +0000</lastBuildDate><image><url>https://jamesdhope.com/media/icon_hu_6b956feff6c8d004.png</url><title>Data Science</title><link>https://jamesdhope.com/tags/data-science/</link></image><item><title>AI Generated Metadata Enrichments for Unstructured Data with IBM Spectrum Discover &amp; watsonx.ai</title><link>https://jamesdhope.com/post/gen-ai-metadata-enrichments/2024-12-4-gen-ai-metadata-enrichments/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://jamesdhope.com/post/gen-ai-metadata-enrichments/2024-12-4-gen-ai-metadata-enrichments/</guid><description>&lt;p>Generative AI has high utility for generating metadata for both structured and unstructured data and is relevant in the storage domain where data discoverability drives the value of data across the enterprise including for downstream AI projects.&lt;/p>
&lt;p>In a recent IBM Client Engineering project we extended IBM Fusion with the Spectrum Discover Fusion SDK to create a data pipeline for AI generated metadata. We created a metadata policy in IBM Fusion to filter images with missing metadata tags and published the image reference to a Kafka topic for the Spectrum Discover Application to consume. We used the watson machine learning SDK with a basic prompt to generate metadata tags associated with the image that catalogued in IBM Fusion. We integrated IBM Knowledge Catalog for enterprise wide data cataloging and watsonx.ai for querying and to enable downstream AI building.&lt;/p>
&lt;p>We deployed the IBM Spectrum Discover Application to OpenShift for a highly scalable, high-throughput data pipeline.&lt;/p>
&lt;h3 id="system-view">System View&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="system view" srcset="
/media/gen-ai-metadata-enrichments_hu_cf3ee9a752913dcc.webp 400w,
/media/gen-ai-metadata-enrichments_hu_11b53461cae0d096.webp 760w,
/media/gen-ai-metadata-enrichments_hu_bd1ef52e38ee9298.webp 1200w"
src="https://jamesdhope.com/media/gen-ai-metadata-enrichments_hu_cf3ee9a752913dcc.webp"
width="760"
height="482"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h3 id="ibm-spectrum-discover-query-builder">IBM Spectrum Discover Query Builder&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="" srcset="
/media/fusion_hu_93ba5dbaf1b82a42.webp 400w,
/media/fusion_hu_80a58a801e7b3156.webp 760w,
/media/fusion_hu_9de62adaf118a14c.webp 1200w"
src="https://jamesdhope.com/media/fusion_hu_93ba5dbaf1b82a42.webp"
width="760"
height="384"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h3 id="example-ibm-spectrum-discover-application">Example IBM Spectrum Discover Application&lt;/h3>
&lt;p>&lt;a href="https://github.com/IBM/Spectrum_Discover_Example_Application" target="_blank" rel="noopener">https://github.com/IBM/Spectrum_Discover_Example_Application&lt;/a>&lt;/p></description></item><item><title>Graph-Driven, LLM-Assisted Virtual Assistant Architecture</title><link>https://jamesdhope.com/post/graph-driven-llm-assisted/2023-10-2-graph-driven-llm-assistant-virtual-assistant/</link><pubDate>Mon, 02 Oct 2023 00:00:00 +0000</pubDate><guid>https://jamesdhope.com/post/graph-driven-llm-assisted/2023-10-2-graph-driven-llm-assistant-virtual-assistant/</guid><description>&lt;p>View the post here: &lt;a href="https://jamesdhope.medium.com/graph-driven-llm-assisted-virtual-assistant-architecture-c1e4857a7040">&lt;a href="https://jamesdhope.medium.com/graph-driven-llm-assisted-virtual-assistant-architecture-c1e4857a7040" target="_blank" rel="noopener">https://jamesdhope.medium.com/graph-driven-llm-assisted-virtual-assistant-architecture-c1e4857a7040&lt;/a>&lt;/a>.&lt;/p></description></item><item><title>Implementation of the Stable Marriage Algorithm to find 'Stable Groups'</title><link>https://jamesdhope.com/post/stable-groups/2018-05-18-stablegroups/</link><pubDate>Fri, 18 May 2018 00:00:00 +0000</pubDate><guid>https://jamesdhope.com/post/stable-groups/2018-05-18-stablegroups/</guid><description>&lt;p>In my previous post, I explained my implementation of the Stable Marriage Algorithm to find stable pairs. Taking this implementation one step further, I wanted to use the Stable Marriage Algorithm iteratively, to join stable pairs to other stable pairs to forms groups (or sets). This post explains my implementation of how I have used the Stable Marriage Algorithm to form groups of stable pairs.&lt;/p>
&lt;h1>Basic Software Architecture&lt;/h1>
&lt;p>We will make use of my implementation of the Stable Marriage Algorithm. As before, we will need class objects for holding the proposer and acceptor preferences, as well as the list of engagements (or pairs). Additionally, we will need a class object to hold the list of pairs for iteration of the algorithm. We will call this the Pools Class and it will hold Pool Class Objects (or the pairs found at each level).&lt;/p>
&lt;p>Another important aspect of this design is that as we will alterate running the Stable Pairs Algorithm over all proposals and then over proposals of proposers that have not been previously matched in a Stable Pair. This will ensure that these unmatched proposers (or orphans) are given a bit more help in being matched in a Stable Pair for each iteration of the algorithm. So the basic architecture will look something like this:&lt;/p>
&lt;img src="https://raw.githubusercontent.com/jamesdhope/jamesdhope.github.io/master/_posts/SA2.jpg">
&lt;h1>Example Data&lt;/h1>
&lt;p>Let&amp;rsquo;s consider the preferences are now set out as as follows. The preferences will form both the Proposers Table and the Acceptors Table (a mirror copy).&lt;/p>
&lt;style type="text/css">
.tg {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-us36{vertical-align:top}
.tg .tg-xxzo{background-color:#c0c0c0;vertical-align:top}
&lt;/style>
&lt;table class="tg">
&lt;tr>
&lt;th class="tg-xxzo">Johnny&lt;/th>
&lt;th class="tg-us36">Barry&lt;/th>
&lt;th class="tg-us36">Charlie&lt;/th>
&lt;th class="tg-us36">Gilly&lt;/th>
&lt;th class="tg-us36">null&lt;/th>
&lt;th class="tg-us36">null&lt;/th>
&lt;/tr>
&lt;tr>
&lt;td class="tg-xxzo">Alphie&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td class="tg-xxzo">Barry&lt;/td>
&lt;td class="tg-us36">Alphie&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td class="tg-xxzo">Charlie&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">Alphie&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td class="tg-xxzo">Danny&lt;/td>
&lt;td class="tg-us36">Charlie&lt;/td>
&lt;td class="tg-us36">Elle&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td class="tg-xxzo">Freddie&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">Gilly&lt;/td>
&lt;td class="tg-us36">Danny&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td class="tg-xxzo">Gilly&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">Johnny&lt;/td>
&lt;td class="tg-us36">Freddie&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td class="tg-xxzo">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;td class="tg-us36">null&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>From this input file, what we would like to be able to produce is a list of sets that is formed from joining stable pairs to other stable pairs. So the sets that would be formed would be as follows:&lt;/p>
&lt;style type="text/css">
.tg {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-us36{vertical-align:top}
.tg .tg-xxzo{background-color:#c0c0c0;vertical-align:top}
&lt;/style>
&lt;table class="tg">
&lt;tr>
&lt;th class="tg-xxzo">Set 1&lt;/th>
&lt;th class="tg-xxzo">Set 2&lt;/th>
&lt;th class="tg-xxzo">Set 3&lt;/th>
&lt;th class="tg-xxzo">Set 4&lt;/th>
&lt;th class="tg-xxzo">Set 5&lt;/th>
&lt;/tr>
&lt;tr>
&lt;th class="tg-us36">Alphie&lt;/th>
&lt;th class="tg-us36">Barry&lt;/th>
&lt;th class="tg-us36">Charlie&lt;/th>
&lt;th class="tg-us36">Danny&lt;/th>
&lt;th class="tg-us36">Gilly&lt;/th>
&lt;/tr>
&lt;tr>
&lt;th class="tg-us36">&lt;/th>
&lt;th class="tg-us36">&lt;/th>
&lt;th class="tg-us36">&lt;/th>
&lt;th class="tg-us36">Elle&lt;/th>
&lt;th class="tg-us36">Johnny&lt;/th>
&lt;/tr>
&lt;tr>
&lt;th class="tg-us36">&lt;/th>
&lt;th class="tg-us36">&lt;/th>
&lt;th class="tg-us36">&lt;/th>
&lt;th class="tg-us36">&lt;/th>
&lt;th class="tg-us36">Freddie&lt;/th>
&lt;/tr>
&lt;/table>
&lt;h1>A Data Structure for holding Stable Pairs&lt;/h1>
&lt;p>Now let&amp;rsquo;s dive into the python. First we will need to make use of some libraries.&lt;/p>
&lt;p>{% highlight python %}
import sys
import csv
import numpy as np
import pandas as pd
import sets
import copy
{% endhighlight %}&lt;/p>
&lt;p>The Pool Class Object will hold all of the Stable Paris that are found for each run of the Stable Pairs Algorithm. The methods for this class object will manage and update the pairings.&lt;/p>
&lt;p>{% highlight python %}&lt;/p>
&lt;h1 id="pool-class--holds-engagements">Pool Class :: holds engagements&lt;/h1>
&lt;p>class Pool:
def &lt;strong>init&lt;/strong>(self, acceptors):
&amp;quot;&amp;quot;&amp;quot;
Construct an array which will hold the engagements. Instatiate each maximum preference number that
&amp;quot;&amp;quot;&amp;quot;
self.engagements = np.empty(shape=len(acceptors))
self.engagements.fill(np.nan)&lt;/p>
&lt;pre>&lt;code>def new_engagement(self,acceptor,proposer):
&amp;quot;&amp;quot;&amp;quot;
Update (replace) the engagement in the pool
&amp;quot;&amp;quot;&amp;quot;
#print(&amp;quot;entering new engagement&amp;quot;)
if proposer in self.engagements:
#print(&amp;quot;new engagement1&amp;quot;)
self.engagements[self.engagements.tolist().index(proposer)] = np.nan
if acceptor in self.engagements:
#print(&amp;quot;new engagement2&amp;quot;)
self.engagements[self.engagements.tolist().index(acceptor)] = np.nan
self.engagements[acceptor-1] = proposer
self.engagements[proposer-1] = acceptor
#print(&amp;quot;new engagement made&amp;quot;)
def is_complete(self):
&amp;quot;&amp;quot;&amp;quot;
Return True if complete
&amp;quot;&amp;quot;&amp;quot;
if (np.isnan(self.engagements).any()):
return False
else:
return True
def not_engaged(self,proposer):
&amp;quot;&amp;quot;&amp;quot;
Return True if engaged otherwise False
&amp;quot;&amp;quot;&amp;quot;
if proposer in self.engagements:
return False
else:
return True
def get_current_engagement(self,acceptor):
&amp;quot;&amp;quot;&amp;quot;
Return the current engagement for a acceptor
&amp;quot;&amp;quot;&amp;quot;
return self.engagements[acceptor-1]
def get_all_engagements(self):
&amp;quot;&amp;quot;&amp;quot;
Return all the current engagements
&amp;quot;&amp;quot;&amp;quot;
return self.engagements
&lt;/code>&lt;/pre>
&lt;p>{% endhighlight %}&lt;/p>
&lt;h1>A Data Structure for holding Preferences&lt;/h1>
&lt;p>The Acceptor Class object will hold all of the preferences. The is_proposal_accepted() is an important method which deserves some attention here.&lt;/p>
&lt;p>For a proposal to be accepted, is must not cause a set size that exceeds the max_set_size. This is handled by the function is_set_size_allowed() which is explained in a bit more detail later. If the proposal would not cause a set size that exceeds the max_set_size then for the proposal to be accepted, one of the two following conditions must be met:&lt;/p>
&lt;ol>
&lt;li>The proposer and acceptor must be either unmatched; they must have listed each other in their preferences; and, they must not have already been paired in a previous iteration of the Stable Marriage Algorithm. Or:&lt;/li>
&lt;li>They must have listed each other in their preferences and the proposal must be better than the current proposal.&lt;/li>
&lt;/ol>
&lt;p>{% highlight python %}&lt;/p>
&lt;h1 id="acceptor-class--holds-the-acceptor-preferences">Acceptor Class :: holds the acceptor preferences&lt;/h1>
&lt;p>class Acceptor:
def &lt;strong>init&lt;/strong>(self,values):
&amp;quot;&amp;quot;&amp;quot;
Construct the acceptor preferences
&amp;quot;&amp;quot;&amp;quot;
self.values = values&lt;/p>
&lt;pre>&lt;code>def get_preference_number(self,acceptor,proposer,null_position):
&amp;quot;&amp;quot;&amp;quot;
Return the preference of the acceptor for the proposer passed. Return 0 if value is null or if the preference is not in the list.
&amp;quot;&amp;quot;&amp;quot;
#if (proposer==null_position) or (acceptor==null_position):
# return 0
if proposer in self.values[acceptor-1]:
return self.values[acceptor-1].index(proposer)+1
else:
return 0
def is_proposal_accepted(self,acceptor,proposer,pool_object,pools_object,null_position,orphan_round,max_set_size,names,preference,acceptors_table,proposer_object):
&amp;quot;&amp;quot;&amp;quot;
If proposer is in accepter preferences return true else return false
&amp;quot;&amp;quot;&amp;quot;
if debug: print(&amp;quot;position of preference in acceptor table (for proposal):&amp;quot;, self.get_preference_number(acceptor,proposer))
if debug: print(&amp;quot;acceptor is currently engaged to:&amp;quot;, pool_object.get_current_engagement(acceptor))
if debug: print(&amp;quot;position of preference in acceptor table (for current engagement):&amp;quot;, self.get_preference_number(acceptor,pool_object.get_current_engagement(acceptor)))
#check if the proposal would create a set size that exceeds the maximum set size and if so return false
if is_set_size_allowed(acceptors_table,pool_object,names,preference,proposer,proposer_object,pools_object,max_set_size)==False:
return False
if orphan_round:
# If Orphan then If Engagements empty accept, Elseif better than current engagement accept; Else reject (i.e. not listed)
if (np.isnan(pool_object.get_current_engagement(acceptor)) and np.isnan(pool_object.get_current_engagement(proposer)) and pools_object.is_orphan(proposer) and pools_object.is_valid_engagement(acceptor,proposer) and (self.get_preference_number(acceptor,proposer,null_position)!=0)):
return True
elif ((pools_object.is_orphan(proposer) and pools_object.is_valid_engagement(acceptor,proposer) and (self.get_preference_number(acceptor,proposer,null_position)!=0) and (self.get_preference_number(acceptor,proposer,null_position) &amp;lt; self.get_preference_number(acceptor,pool_object.get_current_engagement(acceptor),null_position)))):
return True
else:
return False
else:
# Same logic as above but do not restrict to orphans
if (np.isnan(pool_object.get_current_engagement(acceptor)) and np.isnan(pool_object.get_current_engagement(proposer)) and (self.get_preference_number(acceptor,proposer,null_position)!=0) and pools_object.is_valid_engagement(acceptor,proposer)):
return True
elif ((pools_object.is_valid_engagement(acceptor,proposer) and (self.get_preference_number(acceptor,proposer,null_position)!=0) and (self.get_preference_number(acceptor,proposer,null_position) &amp;lt; self.get_preference_number(acceptor,pool_object.get_current_engagement(acceptor),null_position)))):
return True
else:
return False
&lt;/code>&lt;/pre>
&lt;p>{% endhighlight %}&lt;/p>
&lt;p>As before, the Proposers Class Object holds the list of proposals. There is no change here.&lt;/p>
&lt;p>{% highlight python %}&lt;/p>
&lt;h1 id="proposer-class--holds-the-proposer-preferences">Proposer Class :: holds the proposer preferences&lt;/h1>
&lt;p>class Proposer:
def &lt;strong>init&lt;/strong>(self, values):
&amp;quot;&amp;quot;&amp;quot;
Construct the proposer preferences
&amp;quot;&amp;quot;&amp;quot;
self.values = values&lt;/p>
&lt;pre>&lt;code>def get_proposal(self,proposer,iteration):
&amp;quot;&amp;quot;&amp;quot;
Return the acceptor value (proposal to try) for the proposer and iteration passed
&amp;quot;&amp;quot;&amp;quot;
#print(&amp;quot;proposer&amp;quot;,proposer,&amp;quot;iteration&amp;quot;,iteration,&amp;quot;result&amp;quot;,self.values[proposer][iteration])
return self.values[proposer][iteration]
&lt;/code>&lt;/pre>
&lt;p>{% endhighlight %}&lt;/p>
&lt;h1>A Data Structure for holding previously found Stable Pairs&lt;/h1>
&lt;p>The Pools Class Object will hold the list of Pool Class Objects. The is_valid_engagement() is called to check if a member exists in a Stable Pair in a previous Pool Class Object.&lt;/p>
&lt;p>{% highlight python %}&lt;/p>
&lt;h1 id="pools-class--holds-pool-engagement-objects">Pools Class :: holds pool (engagement) objects&lt;/h1>
&lt;p>class Pools:
def &lt;strong>init&lt;/strong>(self):
&amp;quot;&amp;quot;&amp;quot;
Construct the proposer preferences
&amp;quot;&amp;quot;&amp;quot;
self.values = []&lt;/p>
&lt;pre>&lt;code>def length(self):
&amp;quot;&amp;quot;&amp;quot;
Return the length of the pools set
&amp;quot;&amp;quot;&amp;quot;
return len(self.values)
def add_pool(self,pool_object):
&amp;quot;&amp;quot;&amp;quot;
Add pool objects to the Pools object class
&amp;quot;&amp;quot;&amp;quot;
self.values.append(pool_object)
def remove_pool(self):
&amp;quot;&amp;quot;&amp;quot;
Remove last object added to values
&amp;quot;&amp;quot;&amp;quot;
self.values.pop(len(self.values)-1)
def is_valid_engagement(self,acceptor,proposer):
&amp;quot;&amp;quot;&amp;quot;
If already engaged to proposer in previous iteration; otherwise return True
&amp;quot;&amp;quot;&amp;quot;
for i in range(len(self.values)):
if self.values[i].get_current_engagement(acceptor)==proposer:
return False #if found don't allow engagement
return True
def is_orphan(self,proposer):
&amp;quot;&amp;quot;&amp;quot;
Check if the proposer appears on any previous set of engagements (pool) and if so return True to indicate orphan
&amp;quot;&amp;quot;&amp;quot;
for i in range(len(self.values)):
if not np.isnan(self.values[i].get_current_engagement(proposer)):
return False #if found don't allow engagement
return True
def get_pool_object(self,pool_object_number):
&amp;quot;&amp;quot;&amp;quot;
Return the pool object from the pools class
&amp;quot;&amp;quot;&amp;quot;
return self.values[pool_object_number]
&lt;/code>&lt;/pre>
&lt;p>{% endhighlight %}&lt;/p>
&lt;h1>Routines for importing &amp; encoding the preference data&lt;/h1>
&lt;p>Next, we have a few functions that import the preferences from file, encode and decode the input data as strings to integer values.&lt;/p>
&lt;p>{% highlight python %}&lt;/p>
&lt;p>def import_preferences(input_file):
&amp;quot;&amp;quot;&amp;quot;
Read the data from file and return the names and preferences
&amp;quot;&amp;quot;&amp;quot;
with open(input_file) as csvfile:
preferences = []
names = []
reader = csv.reader(csvfile)
print(&amp;quot;\n Input file read from file \n&amp;quot;)
for row in reader:
data = list(row)
print(&amp;quot;{}&amp;quot;.format(data))
preferences.append(data[1:]) &lt;br>
names.append(data[0])
return(preferences[1:], names[1:])&lt;/p>
&lt;p>def encode(names,name):
&amp;quot;&amp;quot;&amp;quot;
Return the index of the name in the list of names
&amp;quot;&amp;quot;&amp;quot;
return names.index(name)+1&lt;/p>
&lt;p>def return_null_position(names):
&amp;quot;&amp;quot;&amp;quot;
Return tye position of the null value
&amp;quot;&amp;quot;&amp;quot;
return names.index(&amp;ldquo;null&amp;rdquo;)+1&lt;/p>
&lt;p>def encode_preferences(preferences,names,no_of_preferences):
&amp;quot;&amp;quot;&amp;quot;
Encode the preferences
&amp;quot;&amp;quot;&amp;quot;
df = pd.DataFrame(data=preferences)&lt;/p>
&lt;pre>&lt;code>for i in range(0,no_of_preferences):
df[i] = df[i].apply(lambda x: encode(names,x))
return(df.values.tolist())
&lt;/code>&lt;/pre>
&lt;p>def dec(names,index):
&amp;quot;&amp;quot;&amp;quot;
Return the index of the name in the list of names
&amp;quot;&amp;quot;&amp;quot;
return names[index-1]&lt;/p>
&lt;p>def decode(pairs,names):
&amp;quot;&amp;quot;&amp;quot;
Decode the engagements
&amp;quot;&amp;quot;&amp;quot;&lt;br>
df = pd.DataFrame(data=pairs)
df[0] = df[0].fillna(-2)&lt;br>
df = pd.DataFrame(data=pairs).astype(int) &lt;br>
df[0] = df[0].apply(lambda x: dec(names,x) if x&amp;gt;0 else None) #decode engagements (ignore negative values)
return(df.values.tolist())&lt;/p>
&lt;p>{% endhighlight %}&lt;/p>
&lt;h1>Routines for forming sets from stable pairs&lt;/h1>
&lt;p>We will some functions to build the Stable Pairs found at each iteration of the Stable Marriage Algorithm into sets. The function build_pairs returns a dataframe with the stable pairs found at each iteration. The function build_groups then loops through all of the pairs and joins these to previously joined pairs. The Python sets library is ideal for this as the union of two sets implicitly removes any duplication.&lt;/p>
&lt;p>{% highlight python %}&lt;/p>
&lt;p>def build_pairs(names,pools_object):
&amp;quot;&amp;quot;&amp;quot;
Build a list of all stable pairs by fetching pairs from each pool_object in the pools_object
&amp;quot;&amp;quot;&amp;quot;
pairs = pd.DataFrame()
pairs[0] = names&lt;/p>
&lt;pre>&lt;code>for i in range(pools_object.length()):
pairs[i+1] = np.array(decode(pools_object.get_pool_object(i).get_all_engagements(), names)).flatten() #pd.DataFrame(data=engagements)
return(pairs)
&lt;/code>&lt;/pre>
&lt;p>def check_if_intersection(set_1,set_2):
&amp;quot;&amp;quot;&amp;quot;
Returns true if an intersection between two lists is found, otherwise false
&amp;quot;&amp;quot;&amp;quot;
set1 = set(set_1)
set2 = set(set_2)&lt;/p>
&lt;pre>&lt;code>if ((set1.intersection(set2) != set()) and (not &amp;quot;null&amp;quot; in set1) and(not &amp;quot;null&amp;quot; in set2)):
return True
else:
return False
&lt;/code>&lt;/pre>
&lt;p>def get_union(set_1,set_2):
&amp;quot;&amp;quot;&amp;quot;
Returns the union of two lists
&amp;quot;&amp;quot;&amp;quot;
set1 = set(set_1)
set2 = set(set_2)
result = set1.union(set2)&lt;/p>
&lt;pre>&lt;code>if &amp;quot;null&amp;quot; in result:
result.remove(&amp;quot;null&amp;quot;)
return result
&lt;/code>&lt;/pre>
&lt;p>def build_groups(names,pairs,pools_object):
&amp;quot;&amp;quot;&amp;quot;
From the pairs data, build lists of sets and return the set lists (provided there are more than 2 columns)
&amp;quot;&amp;quot;&amp;quot;
sets = []
pairs = pairs.fillna(value=&amp;ldquo;null&amp;rdquo;)&lt;/p>
&lt;pre>&lt;code>#only perform traverse if we have a minimum of two columns
if pairs.shape[1]&amp;gt;1:
#create sets for members at first level
for i in range(len(names)):
result = list(get_union(list([pairs[0][i]]),list([pairs[1][i]])))
if result not in sets:
sets.append(result)
#print(&amp;quot;output of level1 join&amp;quot;, sets)
# Now join level2, level3 etc members and maintain the trees
for i in range(2,pools_object.length()+1): #or #pairs.shape[1]
for j in range(len(names)):
index_to_join = -1
index_to_remove = -1
#check if the student is already in a set somewhere else and if so get the index of that set
for k in range(len(sets)):
if pairs[i][j] in sets[k]:
index_to_remove = k
if debug: print(&amp;quot;index to remove&amp;quot;, index_to_remove)
#get the index of the set the student is already in
for k in range(len(sets)):
if pairs[0][j] in sets[k]:
index_to_join = k
if debug: print(&amp;quot;index to join&amp;quot;,index_to_join)
if (index_to_join==index_to_remove):
#print(&amp;quot;idex to join = index to remove&amp;quot;)
pass
elif ((index_to_remove!=-1) and (index_to_join!=-1)):
sets[index_to_join] = list(get_union(sets[index_to_join],sets[index_to_remove]))
sets.pop(index_to_remove)
df = pd.DataFrame(data=sets)
df = df.transpose()
return df
&lt;/code>&lt;/pre>
&lt;p>{% endhighlight %}&lt;/p>
&lt;h1>The Stable Pairs Algorithm&lt;/h1>
&lt;p>The stable_marriage function will call the stable_pairs function over and over again according to the number of iterations defined. For each iteration, the stable_pairs function is called twice. The first time, we attempt for find stable pairs for each member in the dataset. The second time, we attempt to find stable pairs for members in the dataset that have not yet been paired (at any level).&lt;/p>
&lt;p>{% highlight python %}
def stable_pairs(pool_object,pools_object,proposer_object,proposers_table,accepter_object,acceptors_table,no_of_preferences,null_position,orphan_round,max_set_size,names):
&amp;quot;&amp;quot;&amp;quot;
Create stable engagements and return the list
&amp;quot;&amp;quot;&amp;quot;
for preference in range(0,no_of_preferences):
if debug: print(&amp;quot;/n PREFERENCE:&amp;quot;, preference+1)
#print(&amp;ldquo;width&amp;rdquo;,range(len(proposers_table[preference])))
for proposer in range(len(proposers_table)-1):&lt;/p>
&lt;pre>&lt;code> if pool_object.not_engaged(proposer+1):
if debug: print(&amp;quot;PROPOSAL:&amp;quot;, proposer+1, &amp;quot;----&amp;gt;&amp;quot;, proposers_table[proposer][preference])
if accepter_object.is_proposal_accepted(proposer_object.get_proposal(proposer,preference),proposer+1,pool_object,pools_object,null_position,orphan_round,max_set_size,names,preference,acceptors_table,proposer_object): #if proposal is accepter
if debug: print(&amp;quot;PROPOSAL ACCEPTED&amp;quot;)
pool_object.new_engagement(proposer_object.get_proposal(proposer,preference),proposer+1)
else:
if debug: print(&amp;quot;PROPOSAL FAILED&amp;quot;)
#print(pool_object.get_all_engagements())
if pool_object.is_complete():
return pool_object
return pool_object
&lt;/code>&lt;/pre>
&lt;p>def stable_marriage(pools_object,proposer_object,proposers_table,accepter_object,acceptors_table,iteration,no_of_preferences,null_position,max_set_size,names):
&amp;quot;&amp;quot;&amp;quot;
Call the stable_pairs function iteratively and update the Pools object
&amp;quot;&amp;quot;&amp;quot;
for i in range(iteration):
if debug: print(&amp;ldquo;ITERATION:&amp;quot;,i+1)&lt;/p>
&lt;pre>&lt;code> # add pool object with stable pairs
pool_object = Pool(acceptors_table) #create a pool object
pools_object.add_pool(stable_pairs(pool_object,pools_object,proposer_object,proposers_table,accepter_object,acceptors_table,no_of_preferences,null_position,False,max_set_size,names)) #call stable pairs
print(&amp;quot;\n Engagements (all members) found at iteration {} \n {}&amp;quot;.format(i+1,pool_object.get_all_engagements()))
# add pool object with orphans
pool_object = Pool(acceptors_table) #create a pool object
pools_object.add_pool(stable_pairs(pool_object,pools_object,proposer_object,proposers_table,accepter_object,acceptors_table,no_of_preferences,null_position,True,max_set_size,names)) #call stable pairs
print(&amp;quot;\n Engagements (Orpans) found at iteration {} \n {}&amp;quot;.format(i+1,pool_object.get_all_engagements()))
&lt;/code>&lt;/pre>
&lt;p>{% endhighlight %}&lt;/p>
&lt;p>As we find stable_pairs and accept proposals, we must also check that a proposal would not create a set size that exceeds the max_set_size. To do this, we make a deep copy of the Pools and Pool Objects and call the build_groups and build_pairs functions to return a dataframe with the sets. Unfortunately calling this function each time we make a proposal is computationally inefficient but we will need to check.&lt;/p>
&lt;p>{% highlight python %}
def is_set_size_allowed(acceptors_table,pool_object,names,preference,proposer,proposer_object,pools_object,max_set_size):
&amp;quot;&amp;rdquo;&amp;quot;
If engagement creates a set size that exeeds max_set_size return False; otherwise return True
&amp;quot;&amp;quot;&amp;quot;
# create dummy pool and pools
pool_test = Pool(acceptors_table)
pool_test = copy.deepcopy(pool_object) &lt;br>
pool_test.new_engagement(proposer_object.get_proposal(proposer,preference),proposer+1)&lt;/p>
&lt;pre>&lt;code>pools_test = Pools()
pools_test = copy.deepcopy(pools_object)
pools_test.add_pool(pool_test)
if (len(build_groups(names,build_pairs(names,pools_test),pools_test))&amp;gt;max_set_size):
#print(build_groups(names,build_pairs(names,pools_object),pools_object))
return False
del pool_test
del pools_test
return True
&lt;/code>&lt;/pre>
&lt;p>{% endhighlight %}&lt;/p>
&lt;p>Finally we can call the import routines, encode the input data, instantiate the Pools Class Object and call the stable_marriage function to run the algorithm. A few simple tweaks here would make it possible for us to run the program from the Command Line.&lt;/p>
&lt;p>{% highlight python %}&lt;/p>
&lt;p>def main():&lt;/p>
&lt;pre>&lt;code>try:
input_file = &amp;quot;Preferences4.csv&amp;quot;
#input_file = sys.argv[1]
iteration = 2
#iteration = int(sys.argv[2]) # Number of runs of stable pairs algoirthm. Subsequent runs ignore stable pairs already built.
no_of_preferences = 5
#no_of_preferences = int(sys.argv[3]) # Number of preferences to read from input file
max_set_size = 12
#max_set_size = int(sys.argv[4]) # Maximum number in a set
except:
print(&amp;quot;stableGroups.py --[input_file] --[iteration] --[no_of_preferences] --[max_set_size]\n&amp;quot;)
print(&amp;quot;--[input_file] \n input file in csv format \n&amp;quot;)
print(&amp;quot;--[iteration] \n number of runs of Stable Pairs Algorithm \n&amp;quot;)
print(&amp;quot;--[no_of_preferences] \n number of preferences to read from input file \n&amp;quot;)
print(&amp;quot;--[max_set_size] \n maximum size of a set \n&amp;quot;)
sys.exit()
else: pass
output_file_stable_pairs = 'pools.csv'
output_file_sets = 'sets.csv'
# Import and Encode the preferences data
preferences,names = import_preferences(input_file)
preferences = encode_preferences(preferences,names,no_of_preferences)
print(&amp;quot;\n Input file with {} preferences read and encoded successfully as \n {}&amp;quot;.format(no_of_preferences, preferences))
null_position = return_null_position(names)
print(&amp;quot;\n Instantiating Acceptor and Proposer objects with input preference data... \n&amp;quot;)
acceptors_table = preferences
#acceptors_table = [[1,3,2,4],[3,4,1,2],[4,2,3,1],[3,2,1,4]]
proposers_table = acceptors_table
# Instantiate the Acceptor and Proposer class objects
accepter_object = Acceptor(acceptors_table)
proposer_object = Proposer(proposers_table)
print(&amp;quot;\n Instantiating Pool and Pools objects ready to hold engagements... \n&amp;quot;)
# Instantiate the Pools Class object
pools_object = Pools()
# Run the Algorithm
print(&amp;quot;\n Finding Stable Pairs with {} iterations of the algorithm... \n&amp;quot;.format(iteration))
stable_marriage(pools_object,proposer_object,proposers_table,accepter_object,acceptors_table,iteration,no_of_preferences,null_position,max_set_size,names)
# Write the engagements to file
pairs = build_pairs(names,pools_object)
print(&amp;quot;\n Writing pairs to file (1st iteration row 1 - 2, 2nd iteration 1 - 3 etc.)\n {}&amp;quot;.format(pairs))
#print(&amp;quot;\n Writing pairs to file (1st iteration row 1 - 2, 2nd iteration 1 - 3 etc.)\n&amp;quot;)
pairs.to_csv(output_file_stable_pairs)
# Convert engagements to sets and write to file
groups = build_groups(names,pairs,pools_object)
print(&amp;quot;\n Writing sets to file (a set is a column)\n {}&amp;quot;.format(groups))
print(&amp;quot;length of groups&amp;quot;, len(groups))
groups.to_csv(output_file_sets)
&lt;/code>&lt;/pre>
&lt;p>if &lt;strong>name&lt;/strong> == &amp;ldquo;&lt;strong>main&lt;/strong>&amp;rdquo;:
main()&lt;/p>
&lt;p>{% endhighlight %}&lt;/p>
&lt;h1>Running the program&lt;/h1>
&lt;p>The output data we get from the input data above is as follows. Note that in the final dataframe the sets are displayed as columns.&lt;/p>
&lt;p>{% highlight python %}&lt;/p>
&lt;p>Input file read from file&lt;/p>
&lt;p>[&amp;lsquo;Name&amp;rsquo;, &amp;lsquo;Preference_1&amp;rsquo;, &amp;lsquo;Preference_2&amp;rsquo;, &amp;lsquo;Preference_3&amp;rsquo;, &amp;lsquo;Preference_4&amp;rsquo;, &amp;lsquo;Preference_5&amp;rsquo;]
[&amp;lsquo;Johnny&amp;rsquo;, &amp;lsquo;Barry&amp;rsquo;, &amp;lsquo;Charlie&amp;rsquo;, &amp;lsquo;Gilly&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;]
[&amp;lsquo;Alphie&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;]
[&amp;lsquo;Barry&amp;rsquo;, &amp;lsquo;Alphie&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;]
[&amp;lsquo;Charlie&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;lsquo;Alphie&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;]
[&amp;lsquo;Danny&amp;rsquo;, &amp;lsquo;Charlie&amp;rsquo;, &amp;lsquo;Elle&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;]
[&amp;lsquo;Elle&amp;rsquo;, &amp;lsquo;Freddie&amp;rsquo;, &amp;lsquo;Danny&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;]
[&amp;lsquo;Freddie&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;lsquo;Gilly&amp;rsquo;, &amp;lsquo;Danny&amp;rsquo;]
[&amp;lsquo;Gilly&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;lsquo;Johnny&amp;rsquo;, &amp;lsquo;Freddie&amp;rsquo;, &amp;rsquo;null&amp;rsquo;]
[&amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;, &amp;rsquo;null&amp;rsquo;]&lt;/p>
&lt;p>Input file with 5 preferences read and encoded successfully as
[[3, 4, 8, 9, 9], [9, 9, 9, 9, 9], [2, 9, 9, 9, 9], [9, 2, 9, 9, 9], [4, 6, 9, 9, 9], [7, 5, 9, 9, 9], [9, 9, 9, 8, 5], [9, 9, 1, 7, 9], [9, 9, 9, 9, 9]]&lt;/p>
&lt;p>Instantiating Acceptor and Proposer objects with input preference data&amp;hellip;&lt;/p>
&lt;p>Instantiating Pool and Pools objects ready to hold engagements&amp;hellip;&lt;/p>
&lt;p>Finding Stable Pairs with 2 iterations of the algorithm&amp;hellip;&lt;/p>
&lt;p>Engagements (all members) found at iteration 1
[ 8. nan nan nan 6. 5. nan 1. nan]&lt;/p>
&lt;p>Engagements (Orpans) found at iteration 1
[nan nan nan nan nan nan 8. 7. nan]&lt;/p>
&lt;p>Engagements (all members) found at iteration 2
[nan nan nan nan nan nan nan nan nan]&lt;/p>
&lt;p>Engagements (Orpans) found at iteration 2
[nan nan nan nan nan nan nan nan nan]&lt;/p>
&lt;p>Writing pairs to file (1st iteration row 1 - 2, 2nd iteration 1 - 3 etc.)
0 1 2 3 4
0 Johnny Gilly None None None
1 Alphie None None None None
2 Barry None None None None
3 Charlie None None None None
4 Danny Elle None None None
5 Elle Danny None None None
6 Freddie None Gilly None None
7 Gilly Johnny Freddie None None
8 null None None None None&lt;/p>
&lt;p>Writing sets to file (a set is a column)
0 1 2 3 4 5
0 Alphie Barry Charlie Danny Gilly None
1 None None None Elle Johnny None
2 None None None None Freddie None
length of groups 3&lt;/p>
&lt;p>{% endhighlight %}&lt;/p>
&lt;p>So the final output of sets is as follows and we can see that it has observed the very simply list of preferences in our input file:&lt;/p>
&lt;style type="text/css">
.tg {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-us36{vertical-align:top}
.tg .tg-xxzo{background-color:#c0c0c0;vertical-align:top}
&lt;/style>
&lt;table class="tg">
&lt;tr>
&lt;th class="tg-xxzo">Set 1&lt;/th>
&lt;th class="tg-xxzo">Set 2&lt;/th>
&lt;th class="tg-xxzo">Set 3&lt;/th>
&lt;th class="tg-xxzo">Set 4&lt;/th>
&lt;th class="tg-xxzo">Set 5&lt;/th>
&lt;/tr>
&lt;tr>
&lt;th class="tg-us36">Alphie&lt;/th>
&lt;th class="tg-us36">Barry&lt;/th>
&lt;th class="tg-us36">Charlie&lt;/th>
&lt;th class="tg-us36">Danny&lt;/th>
&lt;th class="tg-us36">Gilly&lt;/th>
&lt;/tr>
&lt;tr>
&lt;th class="tg-us36">&lt;/th>
&lt;th class="tg-us36">&lt;/th>
&lt;th class="tg-us36">&lt;/th>
&lt;th class="tg-us36">Elle&lt;/th>
&lt;th class="tg-us36">Johnny&lt;/th>
&lt;/tr>
&lt;tr>
&lt;th class="tg-us36">&lt;/th>
&lt;th class="tg-us36">&lt;/th>
&lt;th class="tg-us36">&lt;/th>
&lt;th class="tg-us36">&lt;/th>
&lt;th class="tg-us36">Freddie&lt;/th>
&lt;/tr>
&lt;/table>
&lt;p>A full version of the code is available to download from my GitHub page &lt;a href="https://github.com/jamesdhope/teaching-lecturing-resources/blob/master/stableGroupsX4.py">here.&lt;/a>&lt;/p>
&lt;p>For more information on my implementation of the Stable Marriage Algorithm DM @jamesdhope or email &lt;a href="mailto:{{ site.email }}">{{ site.email }}&lt;/a>.&lt;/p></description></item><item><title>Implementation of the Stable Marriage Algorithm.</title><link>https://jamesdhope.com/post/stable-marriage/2018-04-12-stable-marriage/</link><pubDate>Thu, 12 Apr 2018 00:00:00 +0000</pubDate><guid>https://jamesdhope.com/post/stable-marriage/2018-04-12-stable-marriage/</guid><description>&lt;p>Ever been faced with the challenge of pairing people based on their individual preferences? I can think of numerous situations in which such a task might arise, for example, in pairing up co-workers for a task or pairing up students for an activity.&lt;/p>
&lt;p>If each student were to (1) list out their peers they would &amp;lsquo;propose&amp;rsquo; to work with (in order of preference), and (2) list out their peers in order of who they would &amp;lsquo;accept&amp;rsquo; proposals from (also in order of preference) the Stable Marriage Algorithm would allow us to find a solution such that it would not be preferable for any two students to swap partners - in other words that the matching found by the algorithm is said to be &amp;lsquo;stable&amp;rsquo;.&lt;/p>
&lt;p>Consider that four students list out who they would like to work with as follows. Let&amp;rsquo;s call this the Proposers Table.&lt;/p>
&lt;style type="text/css">
.tg {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-le8v{background-color:#c0c0c0;vertical-align:top}
.tg .tg-yw4l{vertical-align:top}
&lt;/style>
&lt;table class="tg">
&lt;tr>
&lt;th class="tg-le8v">James&lt;/th>
&lt;th class="tg-yw4l">James&lt;/th>
&lt;th class="tg-yw4l">Floriane&lt;/th>
&lt;th class="tg-yw4l">Jemery&lt;/th>
&lt;th class="tg-yw4l">Matthew&lt;/th>
&lt;/tr>
&lt;tr>
&lt;td class="tg-le8v">Floriane&lt;/td>
&lt;td class="tg-yw4l">Jemery&lt;/td>
&lt;td class="tg-yw4l">Matthew&lt;/td>
&lt;td class="tg-yw4l">James&lt;/td>
&lt;td class="tg-yw4l">Floriane&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td class="tg-le8v">Jeremy&lt;/td>
&lt;td class="tg-yw4l">Matthew&lt;/td>
&lt;td class="tg-yw4l">Floriane&lt;/td>
&lt;td class="tg-yw4l">Jemery&lt;/td>
&lt;td class="tg-yw4l">Jemery&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td class="tg-le8v">Matthew&lt;/td>
&lt;td class="tg-yw4l">Jemery&lt;/td>
&lt;td class="tg-yw4l">Floriane&lt;/td>
&lt;td class="tg-yw4l">James&lt;/td>
&lt;td class="tg-yw4l">Matthew&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>Consider also that the students list out who they would be willing to accept proposals from, in order of preference. Let&amp;rsquo;s call this the Acceptors Table.&lt;/p>
&lt;style type="text/css">
.tg {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-us36{vertical-align:top}
.tg .tg-xxzo{background-color:#c0c0c0;vertical-align:top}
&lt;/style>
&lt;table class="tg">
&lt;tr>
&lt;th class="tg-xxzo">James&lt;/th>
&lt;th class="tg-us36">Floriane&lt;/th>
&lt;th class="tg-us36">James&lt;/th>
&lt;th class="tg-us36">Jemery&lt;/th>
&lt;th class="tg-us36">Matthew&lt;/th>
&lt;/tr>
&lt;tr>
&lt;td class="tg-xxzo">Floriane&lt;/td>
&lt;td class="tg-us36">Matthew&lt;/td>
&lt;td class="tg-us36">James&lt;/td>
&lt;td class="tg-us36">Floriane&lt;/td>
&lt;td class="tg-us36">Jemery&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td class="tg-xxzo">Jeremy&lt;/td>
&lt;td class="tg-us36">James&lt;/td>
&lt;td class="tg-us36">Jemery&lt;/td>
&lt;td class="tg-us36">Floriane&lt;/td>
&lt;td class="tg-us36">Matthew&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td class="tg-xxzo">Matthew&lt;/td>
&lt;td class="tg-us36">Floriane&lt;/td>
&lt;td class="tg-us36">Jemery&lt;/td>
&lt;td class="tg-us36">James&lt;/td>
&lt;td class="tg-us36">Matthew&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>For simplicity, we will allow the students to propose to themselves (i.e. to work with themselves). The proposers and accepters table may be encoded as follows:&lt;/p>
&lt;p>{% highlight python %}
Acceptors Table: [[1, 2, 3, 4], [3, 4, 1, 2], [4, 2, 3, 1], [3, 2, 1, 4]]
Proposers Table: [[2, 1, 3, 4], [4, 1, 2, 3], [1, 3, 2, 4], [2, 3, 1, 4]]
{% endhighlight %}&lt;/p>
&lt;p>Now, diving into the code. I have implemented the algorithm using three classes. First the Pool Class, which will hold and maintain the list of engagements.&lt;/p>
&lt;p>{% highlight python %}&lt;/p>
&lt;h1 id="pool-class--holds-engagements">Pool Class :: holds engagements&lt;/h1>
&lt;p>class Pool:
def &lt;strong>init&lt;/strong>(self, acceptors):
&amp;quot;&amp;quot;&amp;quot;
Construct an array which will hold the engagements. Instatiate each maximum preference number that
&amp;quot;&amp;quot;&amp;quot;
self.engagements = np.empty(shape=len(acceptors))
self.engagements.fill(np.nan)&lt;/p>
&lt;pre>&lt;code>def new_engagement(self,acceptor,proposer):
&amp;quot;&amp;quot;&amp;quot;
Update (replace) the engagement in the pool
&amp;quot;&amp;quot;&amp;quot;
if proposer in self.engagements:
print(proposer, &amp;quot;in position&amp;quot;, self.engagements.tolist().index(proposer)+1, &amp;quot;set to NaN&amp;quot;)
self.engagements[self.engagements.tolist().index(proposer)] = np.nan
self.engagements[acceptor-1] = proposer
def is_complete(self):
&amp;quot;&amp;quot;&amp;quot;
Return True if complete
&amp;quot;&amp;quot;&amp;quot;
if (np.isnan(self.engagements).any()):
return False
else:
return True
def get_current_engagement(self,acceptor):
&amp;quot;&amp;quot;&amp;quot;
Return the current engagement for a acceptor
&amp;quot;&amp;quot;&amp;quot;
return self.engagements[acceptor-1]
def get_all_engagements(self):
&amp;quot;&amp;quot;&amp;quot;
Return all the current engagements
&amp;quot;&amp;quot;&amp;quot;
return self.engagements
&lt;/code>&lt;/pre>
&lt;p>{% endhighlight %}&lt;/p>
&lt;p>The Acceptor Class will hold the preferences of the person being proposed to, return prefereces and decide if a proposal is accepted or rejected, depending on their current engagement status. The is_proposal_accepted method will return True if either the member being proposed to has no current engagement or if the proposing member is higher up in their list of preferences than their current engagement, otherwise it will return False.&lt;/p>
&lt;p>{% highlight python %}&lt;/p>
&lt;h1 id="acceptor-class--holds-the-acceptor-preferences">Acceptor Class :: holds the acceptor preferences&lt;/h1>
&lt;p>class Acceptor:
def &lt;strong>init&lt;/strong>(self,values):
&amp;quot;&amp;quot;&amp;quot;
Construct the acceptor preferences
&amp;quot;&amp;quot;&amp;quot;
self.values = values&lt;/p>
&lt;pre>&lt;code>def get_preference_number(self,acceptor,proposer):
&amp;quot;&amp;quot;&amp;quot;
Return the preference of the acceptor for the proposer passed
&amp;quot;&amp;quot;&amp;quot;
#print(self.values[acceptor-1])
if proposer in self.values[acceptor-1]:
return self.values[acceptor-1].index(proposer)+1
else:
return 0
def is_proposal_accepted(self,acceptor,proposer):
&amp;quot;&amp;quot;&amp;quot;
If proposer is in accepter preferences return true else return false
&amp;quot;&amp;quot;&amp;quot;
if debug: (print(&amp;quot;acceptor preference of proposal&amp;quot;, self.get_preference_number(acceptor,proposer)))
if debug: (print(&amp;quot;acceptor currently engaged to&amp;quot;, pool_object.get_current_engagement(acceptor)))
if debug: (print(&amp;quot;acceptor preference of current engagement&amp;quot;, self.get_preference_number(acceptor,pool_object.get_current_engagement(acceptor))))
if (np.isnan(pool_object.get_current_engagement(acceptor)) and (self.get_preference_number(acceptor,proposer)!=0)):
return True
if (self.get_preference_number(acceptor,proposer) &amp;lt; self.get_preference_number(acceptor,pool_object.get_current_engagement(acceptor))):
return True
else:
return False
&lt;/code>&lt;/pre>
&lt;p>{% endhighlight %}&lt;/p>
&lt;p>The Proposer Class will hold the proposers preferences. The get_proposal method will return the next proposal and will be called until all of the members are engaged.&lt;/p>
&lt;p>{% highlight python %}&lt;/p>
&lt;h1 id="proposer-class--holds-the-proposer-preferences">Proposer Class :: holds the proposer preferences&lt;/h1>
&lt;p>class Proposer:
def &lt;strong>init&lt;/strong>(self, values):
&amp;quot;&amp;quot;&amp;quot;
Construct the proposer preferences
&amp;quot;&amp;quot;&amp;quot;
self.values = values&lt;/p>
&lt;pre>&lt;code>def get_proposal(self,proposer,iteration):
&amp;quot;&amp;quot;&amp;quot;
Return the acceptor value (proposal to try) for the proposer and iteration passed
&amp;quot;&amp;quot;&amp;quot;
#return self.values.iloc[proposer,iteration]
return self.values[proposer][iteration]
&lt;/code>&lt;/pre>
&lt;p>{% endhighlight %}&lt;/p>
&lt;p>Next we call the Pool, Acceptor and Proposer constructor methods to instantiate the class objects. We use the encoded data above.&lt;/p>
&lt;p>{% highlight python %}&lt;/p>
&lt;h1 id="instantiate-the-acceptor-and-proposer-class-passing-the-encoded-data-to-the-class-constructor">Instantiate the Acceptor and Proposer class passing the encoded data to the class constructor&lt;/h1>
&lt;p>accepter_object = Acceptor(acceptors_table)
proposer_object = Proposer(proposers_table)&lt;/p>
&lt;p>print(&amp;ldquo;Acceptors Table:&amp;rdquo;, accepter_object.values)
print(&amp;ldquo;Proposers Table:&amp;rdquo;, proposer_object.values)&lt;/p>
&lt;h1 id="instantiate-the-pool-class">Instantiate the pool class&lt;/h1>
&lt;p>pool_object = Pool(np.unique(acceptors_table))
if debug: print(&amp;ldquo;Pool Object:&amp;rdquo;, pool_object.get_all_engagements())
{% endhighlight %}&lt;/p>
&lt;p>Now the interesting part. We iterate through the proposals in the proposers_table (by row then column) calling the acceptor object each time to determine if a proposal is accepted. If a proposal is accepted then the pool object which maintains the list of engagements is updated. After each iteration, we check if we have a complete one-to-one bipartite mapping (i.e. each member is engaged) and if so we break out and print the final list of engagements. It&amp;rsquo;s that simple.&lt;/p>
&lt;p>{% highlight python %}
def stable_marriage():
for iteration in range(len(proposers_table)):
print(&amp;quot;\n Round:&amp;quot;, iteration+1)
for proposer in range(len(proposers_table[iteration])):
print(&amp;ldquo;PROPOSAL:&amp;rdquo;, proposer+1, &amp;ldquo;&amp;mdash;-&amp;gt;&amp;rdquo;, proposers_table[proposer][iteration]) &lt;br>
if accepter_object.is_proposal_accepted(proposer_object.get_proposal(proposer,iteration),proposer+1): #if proposal is accepter
if debug: print(&amp;ldquo;PROPOSAL ACCEPTED&amp;rdquo;)
pool_object.new_engagement(proposer_object.get_proposal(proposer,iteration),proposer+1)
else:
if debug: print(&amp;ldquo;PROPOSAL FAILED&amp;rdquo;)
print(&amp;ldquo;ENGAGEMENTS:&amp;rdquo;, pool_object.get_all_engagements())&lt;/p>
&lt;pre>&lt;code> if pool_object.is_complete():
return pool_object.get_all_engagements()
&lt;/code>&lt;/pre>
&lt;p>print(&amp;quot;\n FINAL ENGAGEMENTS:&amp;quot;, stable_marriage())
{% endhighlight %}&lt;/p>
&lt;p>When we run the algorithm, this is the output.&lt;/p>
&lt;p>{% highlight python %}&lt;/p>
&lt;p>Round: 1
PROPOSAL: 1 &amp;mdash;-&amp;gt; 2
ENGAGEMENTS: [nan 1. nan nan]
PROPOSAL: 2 &amp;mdash;-&amp;gt; 4
ENGAGEMENTS: [nan 1. nan 2.]
PROPOSAL: 3 &amp;mdash;-&amp;gt; 1
ENGAGEMENTS: [ 3. 1. nan 2.]
PROPOSAL: 4 &amp;mdash;-&amp;gt; 2
ENGAGEMENTS: [ 3. 4. nan 2.]&lt;/p>
&lt;p>Round: 2
PROPOSAL: 1 &amp;mdash;-&amp;gt; 1
ENGAGEMENTS: [ 1. 4. nan 2.]
PROPOSAL: 2 &amp;mdash;-&amp;gt; 1
ENGAGEMENTS: [ 1. 4. nan 2.]
PROPOSAL: 3 &amp;mdash;-&amp;gt; 3
ENGAGEMENTS: [1. 4. 3. 2.]&lt;/p>
&lt;p>FINAL ENGAGEMENTS: [1. 4. 3. 2.]&lt;/p>
&lt;p>{% endhighlight %}&lt;/p>
&lt;p>So, our matchings are that James works with James, Floriane works with Matthew, Jemery works with Jemery, and Matthew works with Floriane (by symmetry). There are no two students in the group that would prefer to swap partners - hence the solution is said to be &amp;lsquo;stable&amp;rsquo;.&lt;/p>
&lt;p>A full version of the code is available to download from my GitHub page &lt;a href="https://github.com/jamesdhope/teaching-lecturing-resources/blob/master/stableGroups.py">here.&lt;/a>&lt;/p>
&lt;p>For more information on my implementation of the Stable Marriage Algorithm DM @jamesdhope or email &lt;a href="mailto:{{ site.email }}">{{ site.email }}&lt;/a>.&lt;/p></description></item><item><title>Zillow's Zestimate, and my ensemble of regressors for highly featured data prediction</title><link>https://jamesdhope.com/post/zillow-ensemble-regressors/2017-08-17-zillow-ensemble/</link><pubDate>Thu, 17 Aug 2017 00:00:00 +0000</pubDate><guid>https://jamesdhope.com/post/zillow-ensemble-regressors/2017-08-17-zillow-ensemble/</guid><description>&lt;p>&amp;ldquo;The Zillow Prize contest competition, sponsored by Zillow, Inc. (Sponsor) is open to all individuals over the age of 18 at the time of entry. The competition will contain two rounds, one public and one private.. Each round will have separate datasets, submission deadlines and instructions on how to participate. The instructions on how to participate in each round are listed below. Capitalized terms used but not defined herein have the meanings assigned to them in the Zillow Prize competition Official Rules.&amp;rdquo;&lt;/p>
&lt;p>For a full description of the competition, datasets, evaluation, prizes visit &lt;a href="https://www.kaggle.com/c/zillow-prize-1" target="_blank">&lt;a href="https://www.kaggle.com/c/zillow-prize-1" target="_blank" rel="noopener">https://www.kaggle.com/c/zillow-prize-1&lt;/a>&lt;/a>&lt;/p>
&lt;p>My first competition entry, a stacked ensemble of regressors for this competition is available here: &lt;a href="https://www.kaggle.com/jamesdhope/zillow-ensemble-of-regressors-0-065" target="_blank">&lt;a href="https://www.kaggle.com/jamesdhope/zillow-ensemble-of-regressors-0-065" target="_blank" rel="noopener">https://www.kaggle.com/jamesdhope/zillow-ensemble-of-regressors-0-065&lt;/a>&lt;a/>&lt;/p>
&lt;p>&lt;b>Short summary&lt;/b>. The stacked ensemble makes use of the SciKit-Learn RandomForestRegressor, ExtraTreesRegressor, GradientBoostRegressor and AdaBoostRegressor, as well as a Support Vector Machine. We also make use of xgboost to perform regression over the features of the first level ensemble and is used to make final predictions on a set of circa 3 million houses, each with 23 features, for 6 points in time (that&amp;rsquo;s 12 million predictions!).&lt;/p>
&lt;p>Whilst there is room for improvement in preprocessing, including optimising strategies for overcoming missing data (for which there is a lot!), and determining the hyperparameters that lead to an optimal model, this machine learning model is easily adapted for making predictions on featured data in any context.&lt;/p>
&lt;p>&lt;b>Now walking through the code in some more detail&amp;hellip;&lt;/b>. The stacked ensemble makes use of the SciKit-Learn RandomForestRegressor, ExtraTreesRegressor, GradientBoostRegressor and AdaBoostRegressor, as well as a Support Vector Machine. We also make use of xgboost to perform regression over the features of the first level ensemble. So we start out by importing the libraries we will need.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Load in our libraries&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">import&lt;/span> &lt;span class="n">pandas&lt;/span> &lt;span class="n">as&lt;/span> &lt;span class="n">pd&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">import&lt;/span> &lt;span class="n">numpy&lt;/span> &lt;span class="n">as&lt;/span> &lt;span class="n">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">import&lt;/span> &lt;span class="n">sklearn&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">import&lt;/span> &lt;span class="n">xgboost&lt;/span> &lt;span class="n">as&lt;/span> &lt;span class="n">xgb&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">import&lt;/span> &lt;span class="n">seaborn&lt;/span> &lt;span class="n">as&lt;/span> &lt;span class="n">sns&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">import&lt;/span> &lt;span class="n">matplotlib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pyplot&lt;/span> &lt;span class="n">as&lt;/span> &lt;span class="n">plt&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">%&lt;/span>&lt;span class="n">matplotlib&lt;/span> &lt;span class="n">inline&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">import&lt;/span> &lt;span class="n">plotly&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">offline&lt;/span> &lt;span class="n">as&lt;/span> &lt;span class="n">py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">py&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">init_notebook_mode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">connected&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">import&lt;/span> &lt;span class="n">plotly&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">graph_objs&lt;/span> &lt;span class="n">as&lt;/span> &lt;span class="n">go&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">import&lt;/span> &lt;span class="n">plotly&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tools&lt;/span> &lt;span class="n">as&lt;/span> &lt;span class="n">tls&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Going to use these 5 base models for the stacking&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">from&lt;/span> &lt;span class="n">sklearn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ensemble&lt;/span> &lt;span class="n">import&lt;/span> &lt;span class="n">RandomForestRegressor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">AdaBoostRegressor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ExtraTreesRegressor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">GradientBoostingRegressor&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">from&lt;/span> &lt;span class="n">sklearn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">svm&lt;/span> &lt;span class="n">import&lt;/span> &lt;span class="n">LinearSVR&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">from&lt;/span> &lt;span class="n">sklearn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cross_validation&lt;/span> &lt;span class="n">import&lt;/span> &lt;span class="n">KFold&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We also need to load in the training and test datasets that Zillow has provided us.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">train = pd.read_csv(&amp;#39;../input/properties_2016.csv&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train_label = pd.read_csv(&amp;#39;../input/train_2016_v2.csv&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ParcelID = train[&amp;#39;parcelid&amp;#39;]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, we will OneHotEncode some of the features. For some features, it makes sense to assume that missing data means a missing feature, so we can map Nan values to 0.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"># OneHotEncoding
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;has_basement&amp;#39;] = train[&amp;#34;basementsqft&amp;#34;].apply(lambda x: 0 if np.isnan(x) else 1).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;hashottuborspa&amp;#39;] = train[&amp;#34;hashottuborspa&amp;#34;].apply(lambda x: 0 if np.isnan(x) else 1).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;has_pool&amp;#39;] = train[&amp;#34;poolcnt&amp;#34;].apply(lambda x: 0 if np.isnan(x) else 1).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;has_airconditioning&amp;#39;] = train[&amp;#34;airconditioningtypeid&amp;#34;].apply(lambda x: 0 if np.isnan(x) else 1).astype(float)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There are some columns which appear to need consolidating into a single feature.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"># Columns to be consolidated
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;yardbuildingsqft17&amp;#39;] = train[&amp;#39;yardbuildingsqft17&amp;#39;].apply(lambda x: 0 if np.isnan(x) else x).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;yardbuildingsqft26&amp;#39;] = train[&amp;#39;yardbuildingsqft26&amp;#39;].apply(lambda x: 0 if np.isnan(x) else x).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;yard_building_square_feet&amp;#39;] = train[&amp;#39;yardbuildingsqft17&amp;#39;].astype(int) + train[&amp;#39;yardbuildingsqft26&amp;#39;].astype(float)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And we can also assume some more friendly feature names.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">train.rename(columns={&amp;#39;fireplacecnt&amp;#39;:&amp;#39;fireplace_count&amp;#39;}, inplace=True)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train.rename(columns={&amp;#39;bedroomcnt&amp;#39;:&amp;#39;bedroom_count&amp;#39;}, inplace=True)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train.rename(columns={&amp;#39;bathroomcnt&amp;#39;:&amp;#39;bathroom_count&amp;#39;}, inplace=True)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train.rename(columns={&amp;#39;calculatedfinishedsquarefeet&amp;#39;:&amp;#39;square_feet&amp;#39;}, inplace=True)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train.rename(columns={&amp;#39;garagecarcnt&amp;#39;:&amp;#39;garage_car_count&amp;#39;}, inplace=True)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train.rename(columns={&amp;#39;garagetotalsqft&amp;#39;:&amp;#39;garage_square_feet&amp;#39;}, inplace=True)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train.rename(columns={&amp;#39;hashottuborspa&amp;#39;:&amp;#39;has_hottub_or_spa&amp;#39;}, inplace=True)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train.rename(columns={&amp;#39;landtaxvaluedollarcnt&amp;#39;:&amp;#39;land_tax&amp;#39;}, inplace=True)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train.rename(columns={&amp;#39;lotsizesquarefeet&amp;#39;:&amp;#39;lot_size_square_feet&amp;#39;}, inplace=True)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train.rename(columns={&amp;#39;taxvaluedollarcnt&amp;#39;:&amp;#39;tax_value&amp;#39;}, inplace=True)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train.rename(columns={&amp;#39;taxamount&amp;#39;:&amp;#39;tax_amount&amp;#39;}, inplace=True)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train.rename(columns={&amp;#39;structuretaxvaluedollarcnt&amp;#39;:&amp;#39;structure_tax_value&amp;#39;}, inplace=True)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train.rename(columns={&amp;#39;yearbuilt&amp;#39;:&amp;#39;year_built&amp;#39;}, inplace=True)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train.rename(columns={&amp;#39;roomcnt&amp;#39;:&amp;#39;room_count&amp;#39;}, inplace=True)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We also need to impute values for missing features. We can impute the median feature value across most features as a starting point.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"># Impute zero for NaN for these features
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;fireplace_count&amp;#39;] = train[&amp;#39;fireplace_count&amp;#39;].apply(lambda x: 0 if np.isnan(x) else x).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># Impute median value for NaN for these features
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;bathroom_count&amp;#39;] = train[&amp;#39;bathroom_count&amp;#39;].fillna(train[&amp;#39;bathroom_count&amp;#39;].median()).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;bedroom_count&amp;#39;] = train[&amp;#39;bedroom_count&amp;#39;].fillna(train[&amp;#39;bedroom_count&amp;#39;].median()).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;room_count&amp;#39;] = train[&amp;#39;room_count&amp;#39;].fillna(train[&amp;#39;room_count&amp;#39;].median()).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;tax_amount&amp;#39;] = train[&amp;#39;tax_amount&amp;#39;].fillna(train[&amp;#39;tax_amount&amp;#39;].median()).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;land_tax&amp;#39;] = train[&amp;#39;land_tax&amp;#39;].fillna(train[&amp;#39;land_tax&amp;#39;].median()).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;tax_value&amp;#39;] = train[&amp;#39;tax_value&amp;#39;].fillna(train[&amp;#39;tax_value&amp;#39;].median()).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;structure_tax_value&amp;#39;] = train[&amp;#39;structure_tax_value&amp;#39;].fillna(train[&amp;#39;structure_tax_value&amp;#39;].median()).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;garage_square_feet&amp;#39;] = train[&amp;#39;garage_square_feet&amp;#39;].fillna(train[&amp;#39;garage_square_feet&amp;#39;].median()).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;garage_car_count&amp;#39;] = train[&amp;#39;garage_car_count&amp;#39;].fillna(train[&amp;#39;garage_car_count&amp;#39;].median()).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;fireplace_count&amp;#39;] = train[&amp;#39;fireplace_count&amp;#39;].fillna(train[&amp;#39;fireplace_count&amp;#39;].median()).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;square_feet&amp;#39;] = train[&amp;#39;square_feet&amp;#39;].fillna(train[&amp;#39;square_feet&amp;#39;].median()).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;year_built&amp;#39;] = train[&amp;#39;year_built&amp;#39;].fillna(train[&amp;#39;year_built&amp;#39;].median()).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;lot_size_square_feet&amp;#39;] = train[&amp;#39;lot_size_square_feet&amp;#39;].fillna(train[&amp;#39;lot_size_square_feet&amp;#39;].median()).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;longitude&amp;#39;] = train[&amp;#39;longitude&amp;#39;].fillna(train[&amp;#39;longitude&amp;#39;].median()).astype(float)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">train[&amp;#39;latitude&amp;#39;] = train[&amp;#39;latitude&amp;#39;].fillna(train[&amp;#39;latitude&amp;#39;].median()).astype(float)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now on to Feature Selection. We will drop features where the volume of missing data exceeds a certain threshold. These features were not considered for imputation above.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Drop indistinct features&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">drop_elements&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;assessmentyear&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Drop any columns insufficiently described&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">drop_elements&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">drop_elements&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;airconditioningtypeid&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;basementsqft&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;architecturalstyletypeid&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;buildingclasstypeid&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;buildingqualitytypeid&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;calculatedbathnbr&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;decktypeid&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;finishedfloor1squarefeet&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;fips&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;heatingorsystemtypeid&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;rawcensustractandblock&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;numberofstories&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;storytypeid&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;threequarterbathnbr&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;typeconstructiontypeid&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;unitcnt&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;censustractandblock&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;fireplaceflag&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;taxdelinquencyflag&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;taxdelinquencyyear&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Drop any duplicated columns&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">drop_elements&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">drop_elements&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;fullbathcnt&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;finishedsquarefeet6&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;finishedsquarefeet12&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;finishedsquarefeet13&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;finishedsquarefeet15&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;finishedsquarefeet50&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;yardbuildingsqft17&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;yardbuildingsqft26&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Land use data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">drop_elements&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">drop_elements&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;propertycountylandusecode&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;propertylandusetypeid&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;propertyzoningdesc&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># We&amp;#39;ll make do with a binary feature here&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">drop_elements&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">drop_elements&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;pooltypeid10&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;pooltypeid2&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;pooltypeid7&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;poolsizesum&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;poolcnt&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># We&amp;#39;ll use the longitude and latitutde as features &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">drop_elements&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">drop_elements&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;regionidzip&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;regionidneighborhood&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;regionidcity&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;regionidcounty&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">train&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">train&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">drop&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">drop_elements&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">axis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can now correlate the features using the Seaborn library Pearson&amp;rsquo;s Correlation. This is ideal for helping with feature reduction as ideally we want as fewer features as possible for regression. We might consider removing some more features here with a high correlation.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="pearson.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>It&amp;rsquo;s also a good idea to scale the data at this point. I&amp;rsquo;ve left this out for brevity but you can refer to the full code if you are unsure how to do this.&lt;/p>
&lt;p>Now a little preparation before we build our models. We&amp;rsquo;ll create an object called SklearnHelper that will extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn classifiers. This cuts out redundancy as won&amp;rsquo;t need to write the same methods five times if we wanted to invoke five different classifiers.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"># Class to extend the Sklearn classifier
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">class SklearnHelper(object):
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> def __init__(self, clf, seed=0, params=None):
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> params[&amp;#39;random_state&amp;#39;] = seed
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> self.clf = clf(**params)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> def train(self, x_train, y_train):
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> self.clf.fit(x_train, y_train)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> def predict(self, x):
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> return self.clf.predict(x)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> def fit(self,x,y):
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> return self.clf.fit(x,y)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> def feature_importances(self,x,y):
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> return(self.clf.fit(x,y).feature_importances_)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We&amp;rsquo;ll also define a function for Cross Validation. This deserves a little explanation. The function will be passed the model, the training set and the test set (for all six time periods). It will make kf=5 folds of the training data, train the model on each fold and make predictions for each time period using this model. It will then take an average of the predicted scores across the five folds for each time period.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">get_oof&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">clf&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x_test_201610&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x_test_201611&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x_test_201612&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x_test_201710&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x_test_201711&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x_test_201712&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_train&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">ntrain&lt;/span>&lt;span class="p">,))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_201610&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">ntest&lt;/span>&lt;span class="p">,))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_201611&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">ntest&lt;/span>&lt;span class="p">,))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_201612&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">ntest&lt;/span>&lt;span class="p">,))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_201710&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">ntest&lt;/span>&lt;span class="p">,))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_201711&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">ntest&lt;/span>&lt;span class="p">,))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_201712&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">ntest&lt;/span>&lt;span class="p">,))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_skf_201610&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">NFOLDS&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ntest&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_skf_201611&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">NFOLDS&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ntest&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_skf_201612&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">NFOLDS&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ntest&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_skf_201710&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">NFOLDS&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ntest&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_skf_201711&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">NFOLDS&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ntest&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_skf_201712&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">NFOLDS&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ntest&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#train_index: indicies of training set&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#test_index: indicies of testing set&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">train_index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_index&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">kf&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#break the dataset down into two sets, train and test&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x_tr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x_train&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">train_index&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">y_tr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">y_train&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">train_index&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x_te&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x_train&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">test_index&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">clf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">train&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x_tr&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_tr&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#make a predition on the test data subset&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_train&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">test_index&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">clf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">predict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x_te&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#use the model trained on the first fold to make a prediction on the entire test data &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_skf_201610&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">clf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">predict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x_test_201610&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_skf_201611&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">clf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">predict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x_test_201611&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_skf_201612&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">clf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">predict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x_test_201612&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_skf_201710&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">clf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">predict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x_test_201710&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_skf_201711&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">clf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">predict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x_test_201711&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_skf_201712&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">clf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">predict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x_test_201712&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#take an average of all of the folds&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_201610&lt;/span>&lt;span class="p">[:]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">oof_test_skf_201610&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_201611&lt;/span>&lt;span class="p">[:]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">oof_test_skf_201611&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_201612&lt;/span>&lt;span class="p">[:]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">oof_test_skf_201612&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_201710&lt;/span>&lt;span class="p">[:]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">oof_test_skf_201710&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_201711&lt;/span>&lt;span class="p">[:]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">oof_test_skf_201711&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">oof_test_201712&lt;/span>&lt;span class="p">[:]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">oof_test_skf_201712&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">oof_train&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">oof_test_201610&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">oof_test_201611&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">oof_test_201612&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">oof_test_201710&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">oof_test_201711&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">oof_test_201712&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next we&amp;rsquo;ll create a Dict data type to hold all of our model parameters.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">SEED = 0 # for reproducibility
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NFOLDS = 5 # set folds for out-of-fold prediction
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># Put in our parameters for said classifiers
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># Random Forest parameters
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">rf_params = {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;n_jobs&amp;#39;: -1,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;n_estimators&amp;#39;: 500,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;warm_start&amp;#39;: True,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> #&amp;#39;max_features&amp;#39;: 0.2,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;max_depth&amp;#39;: 6,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;min_samples_leaf&amp;#39;: 2,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;max_features&amp;#39; : &amp;#39;sqrt&amp;#39;,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;verbose&amp;#39;: 0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># Extra Trees Parameters
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">et_params = {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;n_jobs&amp;#39;: -1,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;n_estimators&amp;#39;:500,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> #&amp;#39;max_features&amp;#39;: 0.5,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;max_depth&amp;#39;: 8,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;min_samples_leaf&amp;#39;: 2,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;verbose&amp;#39;: 0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># AdaBoost parameters
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ada_params = {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;n_estimators&amp;#39;: 400,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;learning_rate&amp;#39; : 0.75
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># Gradient Boosting parameters
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">gb_regressor_params = {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;n_estimators&amp;#39;:500,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;learning_rate&amp;#39;:0.1,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;max_depth&amp;#39;:1,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;random_state&amp;#39;:0,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;loss&amp;#39;:&amp;#39;ls&amp;#39;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We&amp;rsquo;ll now create our models.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">rf = SklearnHelper(clf=RandomForestRegressor, seed=SEED, params=rf_params)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">et = SklearnHelper(clf=ExtraTreesRegressor, seed=SEED, params=et_params)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ada = SklearnHelper(clf=AdaBoostRegressor, seed=SEED, params=ada_params)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">gb_regressor = SklearnHelper(clf=GradientBoostingRegressor, seed=SEED, params=gb_regressor_params)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And now train the models&amp;hellip;&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">et_oof_train, et_oof_test_201610, et_oof_test_201611, et_oof_test_201612, et_oof_test_201710, et_oof_test_201711, et_oof_test_201712 = get_oof(et, x_train, y_train, x_test_201610, x_test_201611, x_test_201612, x_test_201710, x_test_201711, x_test_201712) # Extra Trees
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">rf_oof_train, rf_oof_test_201610, rf_oof_test_201611, rf_oof_test_201612, rf_oof_test_201710, rf_oof_test_201711, rf_oof_test_201712 = get_oof(rf,x_train, y_train, x_test_201610, x_test_201611, x_test_201612, x_test_201710, x_test_201711, x_test_201712) # Random Forest
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ada_oof_train, ada_oof_test_201610, ada_oof_test_201611, ada_oof_test_201612, ada_oof_test_201710, ada_oof_test_201711, ada_oof_test_201712 = get_oof(ada, x_train, y_train, x_test_201610, x_test_201611, x_test_201612, x_test_201710, x_test_201711, x_test_201712) # AdaBoost
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">gb_regressor_oof_train, gb_regressor_oof_test_201610, gb_regressor_oof_test_201611, gb_regressor_oof_test_201612, gb_regressor_oof_test_201710, gb_regressor_oof_test_201711, gb_regressor_oof_test_201712 = get_oof(gb_regressor,x_train,y_train,x_test_201610, x_test_201611, x_test_201612, x_test_201710, x_test_201711, x_test_201712)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, with the models trained, we have now reached the end of the first layer of our ensemble. We can now extract the features for further analysis.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">rf_feature = rf.feature_importances(x_train,y_train)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">print(&amp;#34;rf_feature&amp;#34;, rf_feature)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">et_feature = et.feature_importances(x_train, y_train)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">print(&amp;#34;et_feature&amp;#34;, et_feature)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ada_feature = ada.feature_importances(x_train, y_train)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">print(&amp;#34;ada_feature&amp;#34;, ada_feature)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">gb_regressor_feature = gb_regressor.feature_importances(x_train,y_train)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">print(&amp;#34;gb_regressor_feature&amp;#34;, gb_regressor_feature)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The 23 features we obtain for each model are as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">rf_feature [ 0.04038533 0.02947441 0.14908661 0.0023588 0.00515421 0.01727217
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 0.0020252 0.07555324 0.07418552 0.06010003 0.01318217 0.04547284
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 0.11738776 0.09638334 0.07514663 0.11330465 0.00048846 0.00700142
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 0.00589092 0.00323037 0. 0.03016362 0.03675231]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">et_feature [ 0.06465583 0.0572915 0.12032578 0.00991171 0.01124228 0.00960876
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 0.01485536 0.06740794 0.05175181 0.05436677 0.01772004 0.05594463
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 0.09801529 0.0533328 0.0450343 0.08253611 0.00201233 0.02357432
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 0.03032919 0.00296583 0. 0.061361 0.06575642]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ada_feature [ 8.36785346e-03 3.79894667e-03 7.05391914e-02 7.82563418e-05
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 3.22502690e-07 1.36595920e-02 0.00000000e+00 6.15640675e-02
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 5.32715094e-02 3.73193212e-02 1.70693107e-02 1.18344505e-01
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1.72005440e-01 3.48224492e-02 4.48032666e-02 3.87885107e-02
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 0.00000000e+00 7.54103834e-03 0.00000000e+00 1.06703836e-02
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 0.00000000e+00 1.25617605e-01 1.81738430e-01]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">gb_regressor_feature [ 0.02 0.012 0.246 0. 0.016 0.002 0.004 0.114 0.068 0.02
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 0.004 0.012 0.158 0.056 0.06 0.16 0. 0.022 0. 0. 0.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 0.026 0. ]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s have a look at how important these features are for each model.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="newplot_1.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="newplot_2.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="newplot_3.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="newplot_4.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Now, across the four models, the mean feature importances.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="newplot_5.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>We can now build a new dataframe to hold these features, and train a regression model using xgboost on these features as our second layer.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">#predictions from first layer become data input for second layer
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">base_predictions_train = pd.DataFrame(
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;RandomForest&amp;#39;: rf_oof_train.ravel(),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;ExtraTrees&amp;#39;: et_oof_train.ravel(),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;AdaBoost&amp;#39;: ada_oof_train.ravel(),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;GradientRegressor&amp;#39;: gb_regressor_oof_train.ravel()
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> })
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">#predictions for all instances in the training set
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">base_predictions_train.head(3)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">gbm = xgb.XGBRegressor(
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> #learning_rate = 0.02,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> n_estimators= 2000,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> max_depth= 4,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> min_child_weight= 2,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> #gamma=1,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> gamma=0.9,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> subsample=0.8,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> colsample_bytree=0.8,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> objective= &amp;#39;reg:linear&amp;#39;,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> nthread= -1,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> scale_pos_weight=1
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ).fit(x_train, y_train)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now to make our final predictions&amp;hellip;&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">predictions_201610 = gbm.predict(x_test_201610).round(4)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">predictions_201611 = gbm.predict(x_test_201611).round(4)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">predictions_201612 = gbm.predict(x_test_201612).round(4)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">predictions_201710 = gbm.predict(x_test_201710).round(4)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">predictions_201711 = gbm.predict(x_test_201711).round(4)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">predictions_201712 = gbm.predict(x_test_201712).round(4)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">StackingSubmission = pd.DataFrame({ &amp;#39;201610&amp;#39;: predictions_201610,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;201611&amp;#39;: predictions_201611,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;201612&amp;#39;: predictions_201612,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;201710&amp;#39;: predictions_201710,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;201711&amp;#39;: predictions_201711,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;201712&amp;#39;: predictions_201712,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &amp;#39;ParcelId&amp;#39;: ParcelID,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> })
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">print(StackingSubmission)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item></channel></rss>