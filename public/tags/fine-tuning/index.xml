<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Fine Tuning | James Hope</title><link>https://jamesdhope.com/tags/fine-tuning/</link><atom:link href="https://jamesdhope.com/tags/fine-tuning/index.xml" rel="self" type="application/rss+xml"/><description>Fine Tuning</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 25 Nov 2024 00:00:00 +0000</lastBuildDate><image><url>https://jamesdhope.com/media/icon_hu_6b956feff6c8d004.png</url><title>Fine Tuning</title><link>https://jamesdhope.com/tags/fine-tuning/</link></image><item><title>Operating AI at Scale with OpenShiftAI, KubeFlow Pipelines and watsonx</title><link>https://jamesdhope.com/post/operating-ai-at-scale/2024-11-25-operating-ai-at-scale/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://jamesdhope.com/post/operating-ai-at-scale/2024-11-25-operating-ai-at-scale/</guid><description>&lt;p>Operating AI across different clouds and execution engines becomes complex and difficult to maintain with cloud native tools as the number of different integrations between systems proliferates at scale. OpenShiftAI provides a cohesive hybrid, multi-cloud AI platform that enables enterprises to separate concerns between pipeline orchestration and workload execution reducing complexity in the data and governance subdomains and enabling enterprises to operate AI at scale.&lt;/p>
&lt;h3 id="functions-of-an-ai-operations-system">Functions of an AI Operations System&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Functional View of AI Operations" srcset="
/media/AIOps_1_hu_c5672f5c18ae659d.webp 400w,
/media/AIOps_1_hu_bc1afd2648cb1d4d.webp 760w,
/media/AIOps_1_hu_f4e5cd19a2b5f03b.webp 1200w"
src="https://jamesdhope.com/media/AIOps_1_hu_c5672f5c18ae659d.webp"
width="760"
height="294"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h3 id="openshiftai-watsonxdata--watsonxgovernance-enabling-ai-at-scale">OpenShiftAI, watsonx.data &amp;amp; watsonx.governance enabling AI at Scale&lt;/h3>
&lt;p>OpenShiftAI combined with watsonx.data and watsonx.governance enables enterprise AI at scale in the following ways:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>separation of concerns between pipeline orchestration and training/serving workload execution, demonstrating workload placement to where it makes sense, for reasons such as data compliance or service level agreements for downstream AI&lt;/p>
&lt;/li>
&lt;li>
&lt;p>versioning and orchestration of pipelines as a hybrid multicloud platform-first approach, removing the need for and complexity that results from cloud native integrations that proliferate in number when operating AI at scale, and unlocking the potential to operate AI across the enterprise&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pipelines for super fine tuning an open language model (we show LoRA PEFT fine tuning with IBM hashtag#Granite but this is easily extensible to full SFT or model distillation), because small open models are the future for enterprise AI&lt;/p>
&lt;/li>
&lt;li>
&lt;p>distributing training and observability of GPU workloads with Ray, because distributed compute is important if not essential for operating AI at scale&lt;/p>
&lt;/li>
&lt;li>
&lt;p>watsonx.data as a cloud agnostic feature store, because data is disparate and AI builders need that data to derive value for the enterprise&lt;/p>
&lt;/li>
&lt;li>
&lt;p>publication of model factsheets in watsonx.governance and tracking models as part of an AI Use Case, because enterprise AI needs to be governed.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="integrating-watsonxgovernance-with-openshiftai-kubeflow-pipelines">Integrating watsonx.governance with OpenShiftAI KubeFlow Pipelines&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="OpenShiftAI integration with watsonx.governance" srcset="
/media/AIOps_2_hu_b7c0426947db4dbc.webp 400w,
/media/AIOps_2_hu_1cdd30a0962862f9.webp 760w,
/media/AIOps_2_hu_f60222e1f4f27b5a.webp 1200w"
src="https://jamesdhope.com/media/AIOps_2_hu_b7c0426947db4dbc.webp"
width="760"
height="453"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>For a more in-depth review of OpenShiftAI and Kubeflow pipelines see: &lt;a href="https://blog.pierswalter.co.uk/posts/openshift-ai-pipeline/" target="_blank" rel="noopener">https://blog.pierswalter.co.uk/posts/openshift-ai-pipeline/&lt;/a>&lt;/p></description></item><item><title>Supervised fine tuning of a large language model using quantized low rank adapters</title><link>https://jamesdhope.com/post/fine-tuning-lora/2023-12-01-lora-fine-tuning/</link><pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate><guid>https://jamesdhope.com/post/fine-tuning-lora/2023-12-01-lora-fine-tuning/</guid><description>&lt;p>Fine-tuning of a large language model (LLM) can be peformed using QLoRA (Quantized Low Rank Adapters) and PEFT (Parameter-Efficient Fine-Tuning) techniques.&lt;/p>
&lt;p>PEFT (Parameter-Efficient Fine-Tuning):&lt;/p>
&lt;ul>
&lt;li>PEFT is a technique for fine-tuning large language models with a small number of additional parameters, known as adapters, while freezing the original model parameters.&lt;/li>
&lt;li>It allows for efficient fine-tuning of language models, reducing the memory footprint and computational requirements.&lt;/li>
&lt;li>PEFT enables the injection of niche expertise into a foundation model without catastrophic forgetting, preserving the original model&amp;rsquo;s performance.&lt;/li>
&lt;/ul>
&lt;p>LoRA (Low Rank Adapters):&lt;/p>
&lt;ul>
&lt;li>LoRA is a technique that introduces low-rank adapters for fine-tuning large language models, allowing for efficient backpropagation of gradients through a frozen, quantized pretrained model.&lt;/li>
&lt;li>It involves configuring parameters such as attention dimension, alpha parameter for scaling, dropout probability, and task type for the language model.&lt;/li>
&lt;li>LoRA aims to reduce memory usage and computational requirements during fine-tuning, making it possible to train large models on a single GPU while preserving performance.&lt;/li>
&lt;/ul>
&lt;p>These techniques, when combined, enable the efficient fine-tuning of large language models, making the process more accessible and resource-efficient for researchers and practitioners.&lt;/p>
&lt;p>For more information on LoRA refer to: &lt;a href="https://arxiv.org/abs/2305.14314" target="_blank" rel="noopener">https://arxiv.org/abs/2305.14314&lt;/a>&lt;/p>
&lt;p>For a code example refer to: &lt;a href="https://github.com/jamesdhope/LLM-fine-tuning/blob/main/tuning.py" target="_blank" rel="noopener">https://github.com/jamesdhope/LLM-fine-tuning/blob/main/tuning.py&lt;/a>&lt;/p>
&lt;p>Code Attribution: Maxime Labonne&lt;/p></description></item></channel></rss>