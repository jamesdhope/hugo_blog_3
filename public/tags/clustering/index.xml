<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Clustering | Hugo Academic CV Theme</title>
    <link>http://localhost:60966/tags/clustering/</link>
      <atom:link href="http://localhost:60966/tags/clustering/index.xml" rel="self" type="application/rss+xml" />
    <description>Clustering</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sun, 30 Mar 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:60966/media/icon_hu8409542655044666735.png</url>
      <title>Clustering</title>
      <link>http://localhost:60966/tags/clustering/</link>
    </image>
    
    <item>
      <title>SVD for constructing semantic knowledge graphs, semantic retrieval and reasoning</title>
      <link>http://localhost:60966/post/semantic-knowledge-graph/2025-03-30-semantic-knowledge-graph/</link>
      <pubDate>Sun, 30 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:60966/post/semantic-knowledge-graph/2025-03-30-semantic-knowledge-graph/</guid>
      <description>&lt;p&gt;Singular Value Decomposion (SVD) is a well known method for latent semantic analysis. When applied to BERT contextual embeddings SVD produces three components: U, Σ, and V. The eigenvectors in V represent distinct semantic patterns - each one captures a different aspect of meaning in the text. The eigenvalues in Σ tell us how significant each pattern is, effectively showing us what is semantically important and where the semantic &amp;ldquo;holes&amp;rdquo; are - the gaps in meaning that separate different semantic clusters. This elegant mathematical decomposition reveals the fundamental building blocks of meaning in text, creating a natural hierarchy of semantic patterns that can be analyzed through linear algebra and externalised a semantic knowledge graph.&lt;/p&gt;
&lt;p&gt;The advantage of this approach is that the semantic structure emerges naturally from these mathematical properties. We don&amp;rsquo;t need to artificially construct relationships between semantic components - they&amp;rsquo;re already encoded in the eigenvectors. By computing correlations between these eigenvectors, we can identify which semantic patterns are related and which are distinct, creating a natural semantic graph that represents genuine semantic relationships.&lt;/p&gt;
&lt;p&gt;This mathematical foundation opens up powerful possibilities for semantic analysis. The graph structure is a linear expression of the semantic relationships, which means we can perform various linear operations on it. We can analyze the spectral properties of the graph to understand its structure, use matrix operations to identify semantic clusters, or apply other linear algebraic techniques to explore the semantic space.&lt;/p&gt;
&lt;p&gt;The result is a complete semantic analysis that reveals not just what the text means, but how its meanings are structured and related. This mathematical view of semantics - from individual sentences to semantic patterns to pattern relationships - provides a framework for understanding and manipulating semantic relationships through linear algebra.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;semantic-knowledge-graph&#34; srcset=&#34;
               /media/semantic_knowledge_graph_hu9799583382179802691.webp 400w,
               /media/semantic_knowledge_graph_hu9491578576356301513.webp 760w,
               /media/semantic_knowledge_graph_hu5459435713842414010.webp 1200w&#34;
               src=&#34;http://localhost:60966/media/semantic_knowledge_graph_hu9799583382179802691.webp&#34;
               width=&#34;755&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;T&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
